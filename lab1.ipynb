{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочную и тестовую выборки, обучим простую модель k-ближайших соседей с 3мя соседями и оценим её качество на тестовых данных по нескольким метрикам: точность (accuracy), точность (precision), полнота (recall) и F1-мера (F1 Score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline KNN Classification Metrics:\n",
      "Accuracy: 0.9298245614035088\n",
      "Precision: 0.9315068493150684\n",
      "Recall: 0.9577464788732394\n",
      "F1 Score: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных для классификации\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки для классификации\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Бейзлайн модель без оптимизации\n",
    "knn_classifier_baseline = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Оценка бейзлайн модели\n",
    "y_pred_baseline = knn_classifier_baseline.predict(X_test)\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "precision_baseline = precision_score(y_test, y_pred_baseline)\n",
    "recall_baseline = recall_score(y_test, y_pred_baseline)\n",
    "f1_baseline = f1_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Baseline KNN Classification Metrics:\\nAccuracy: {accuracy_baseline}\\nPrecision: {precision_baseline}\\nRecall: {recall_baseline}\\nF1 Score: {f1_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно также разделяем данные на тренировочную и тестовую выборки, выполняем стандартизацию признаков, обучаем модель регрессии k-ближайших соседей с 3мя соседями и оцениваем её производительность на тестовых данных с использованием метрик средней абсолютной ошибки (MAE), среднеквадратичной ошибки (MSE) и R^2 (коэффициент детерминации). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline KNN Regression Metrics:\n",
      "MAE: 0.4599198611111111\n",
      "MSE: 0.4666634350517549\n",
      "R-squared: 0.6438795499720962\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных для регрессии\n",
    "boston = fetch_california_housing()\n",
    "X_boston, y_boston = boston.data, boston.target\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки для регрессии\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)\n",
    "\n",
    "# Стандартизация данных для регрессии\n",
    "scaler = StandardScaler()\n",
    "X_train_boston = scaler.fit_transform(X_train_boston)\n",
    "X_test_boston = scaler.transform(X_test_boston)\n",
    "\n",
    "# Создание и обучение модели KNN для регрессии\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_regressor.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Оценка модели для регрессии\n",
    "y_pred_boston = knn_regressor.predict(X_test_boston)\n",
    "mae = mean_absolute_error(y_test_boston, y_pred_boston)\n",
    "mse = mean_squared_error(y_test_boston, y_pred_boston)\n",
    "r2 = r2_score(y_test_boston, y_pred_boston)\n",
    "\n",
    "print(f\"\\nBaseline KNN Regression Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nR-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим пайплайн для стандартизации данных и примененим KNN-классификатор, а затем использует метод GridSearchCV для поиска наилучших гиперпараметров по сетке параметров с перекрестной проверкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved KNN Classification Metrics:\n",
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.971830985915493\n",
      "Recall: 0.971830985915493\n",
      "F1 Score: 0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "## Улучшенная модель с оптимизацией\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]  # Манхэттенское или Евклидово расстояние\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Обучаем улучшенную модель\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Оценка улучшенной модели\n",
    "y_pred_improved = best_classifier.predict(X_test)\n",
    "accuracy_improved = accuracy_score(y_test, y_pred_improved)\n",
    "precision_improved = precision_score(y_test, y_pred_improved)\n",
    "recall_improved = recall_score(y_test, y_pred_improved)\n",
    "f1_improved = f1_score(y_test, y_pred_improved)\n",
    "\n",
    "print(f\"Improved KNN Classification Metrics:\\nAccuracy: {accuracy_improved}\\nPrecision: {precision_improved}\\nRecall: {recall_improved}\\nF1 Score: {f1_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим пайплайн, который включает в себя:\n",
    "\n",
    "- Заполнение пропущенных значений с помощью медианы.\n",
    "- Стандартизацию данных.\n",
    "- Создание полиномиальных признаков второй степени.\n",
    "- Применение KNeighborsRegressor.\n",
    "\n",
    "Далее оптимизируем гиперпараметры модели KNN с помощью GridSearchCV и перекрестной проверки. Обучаим модель с наилучшими параметрами, найденными в ходе оптимизации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved KNN Regression Metrics:\n",
      "MAE: 0.41595939485911765\n",
      "MSE: 0.38251670809373284\n",
      "R-squared: 0.7080936452318685\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных для регрессии\n",
    "boston = fetch_california_housing()\n",
    "X_boston, y_boston = boston.data, boston.target\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)\n",
    "\n",
    "# Пайплайн для регрессии\n",
    "pipeline_boston = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# Подбор гиперпараметров для регрессии\n",
    "param_grid_reg = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]  # Манхэттенское или Евклидово расстояние\n",
    "}\n",
    "\n",
    "grid_search_boston = GridSearchCV(pipeline_boston, param_grid_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_boston.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Обучаем модель, используя лучшие гиперпараметры\n",
    "best_regressor = grid_search_boston.best_estimator_\n",
    "\n",
    "# Оценка модели для регрессии\n",
    "y_pred_boston = best_regressor.predict(X_test_boston)\n",
    "mae = mean_absolute_error(y_test_boston, y_pred_boston)\n",
    "mse = mean_squared_error(y_test_boston, y_pred_boston)\n",
    "r2 = r2_score(y_test_boston, y_pred_boston)\n",
    "\n",
    "print(f\"\\nImproved KNN Regression Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nR-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем кастомный KNN для решения задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CustomKNNClassifier:\n",
    "    def __init__(self, n_neighbors=3):\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Вычисление расстояний от x до всех точек в обучающем наборе\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        # Получение k ближайших точек\n",
    "        k_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Определение наиболее встречающегося класса\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KNN Classification Metrics:\n",
      "Accuracy: 0.9298245614035088\n",
      "Precision: 0.9315068493150684\n",
      "Recall: 0.9577464788732394\n",
      "F1 Score: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных и разделение на обучающий и тестовый наборы\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение пользовательского KNN-классификатора\n",
    "knn = CustomKNNClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Оценка\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Custom KNN Classification Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем кастомный KNN для решения задачи регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNNRegressor:\n",
    "    def __init__(self, n_neighbors=3):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Вычисление расстояний от x до всех точек в обучающем наборе\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        # Получение k ближайших точек\n",
    "        k_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Возвращаем среднее значение соседей (для регрессии)\n",
    "        return np.mean(k_nearest_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom KNN Regression Metrics:\n",
      "MAE: 0.8304867425710595\n",
      "MSE: 1.1694144088518572\n",
      "R-squared: 0.10759585116572867\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "boston = fetch_california_housing()\n",
    "X_boston, y_boston = boston.data, boston.target\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение кастомного KNN-регрессора\n",
    "custom_knn_regressor = CustomKNNRegressor(n_neighbors=3)\n",
    "custom_knn_regressor.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Предсказания и оценка модели\n",
    "y_pred_boston = custom_knn_regressor.predict(X_test_boston)\n",
    "mae = mean_absolute_error(y_test_boston, y_pred_boston)\n",
    "mse = mean_squared_error(y_test_boston, y_pred_boston)\n",
    "r2 = r2_score(y_test_boston, y_pred_boston)\n",
    "\n",
    "print(f\"\\nCustom KNN Regression Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nR-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного расширим ранее созданный класс `CustomKNNClassifier`. Дополним:\n",
    "\n",
    "- fit: стандартизирует тренировочные данные перед обучением модели.\n",
    "- predict: стандартизирует тестовые данные перед выполнением предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Custom KNN Classification Metrics:\n",
      "Accuracy: 0.9473684210526315\n",
      "Precision: 0.9577464788732394\n",
      "Recall: 0.9577464788732394\n",
      "F1 Score: 0.9577464788732394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class ImprovedCustomKNNClassifier(CustomKNNClassifier):\n",
    "    def fit(self, X, y):\n",
    "        # Стандартизация данных\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        super().fit(X_scaled, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return super().predict(X_scaled)\n",
    "\n",
    "# Используем улучшенную версию\n",
    "improved_knn = ImprovedCustomKNNClassifier(n_neighbors=3)\n",
    "improved_knn.fit(X_train, y_train)\n",
    "y_pred_improved = improved_knn.predict(X_test)\n",
    "\n",
    "# Оценка\n",
    "accuracy_improved = accuracy_score(y_test, y_pred_improved)\n",
    "precision_improved = precision_score(y_test, y_pred_improved)\n",
    "recall_improved = recall_score(y_test, y_pred_improved)\n",
    "f1_improved = f1_score(y_test, y_pred_improved)\n",
    "\n",
    "print(f\"Improved Custom KNN Classification Metrics:\\nAccuracy: {accuracy_improved}\\nPrecision: {precision_improved}\\nRecall: {recall_improved}\\nF1 Score: {f1_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также улучшим раннее написанный регрессор. Дополним:\n",
    "\n",
    "Будем использовать StandardScaler для стандартизации признаков (среднее 0, стандартное отклонение 1), что помогает алгоритму KNN работать более эффективно на данных с разными масштабами.\n",
    "\n",
    "Добавим параметр p, который позволяет выбрать степень нормы, используемой для вычисления расстояний (по умолчанию Евклидово расстояние p=2. Можно выбрать Манхэттенское расстояние с p=1 и другие)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCustomKNNRegressor:\n",
    "    def __init__(self, n_neighbors=3, p=2):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Стандартизация данных\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        self.X_train = self.scaler.transform(X)\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = [self._predict(x) for x in X_scaled]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Вычисление расстояний от x до всех точек в обучающем наборе\n",
    "        distances = [np.linalg.norm(x - x_train, ord=self.p) for x_train in self.X_train]\n",
    "        # Получение k ближайших точек\n",
    "        k_indices = np.argsort(distances)[:self.n_neighbors]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Возвращаем среднее значение соседей (для регрессии)\n",
    "        return np.mean(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Custom KNN Regression Metrics:\n",
      "MAE: 0.4599198611111111\n",
      "MSE: 0.4666634350517549\n",
      "R-squared: 0.6438795499720962\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "boston = fetch_california_housing()\n",
    "X_boston, y_boston = boston.data, boston.target\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение улучшенного кастомного KNN-регрессора\n",
    "improved_knn_regressor = ImprovedCustomKNNRegressor(n_neighbors=3, p=2)\n",
    "improved_knn_regressor.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Предсказания и оценка модели\n",
    "y_pred_boston = improved_knn_regressor.predict(X_test_boston)\n",
    "mae = mean_absolute_error(y_test_boston, y_pred_boston)\n",
    "mse = mean_squared_error(y_test_boston, y_pred_boston)\n",
    "r2 = r2_score(y_test_boston, y_pred_boston)\n",
    "\n",
    "print(f\"\\nImproved Custom KNN Regression Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nR-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод по первой лабораторной работе:**\n",
    "\n",
    "- лучшая модель для решения задачи классификации при помощи алгоритма KNN - улучшенный бейзлайн из библиотеки.\n",
    "- лучшая модель для решения задачи регрессии при помощи алгоритма KNN - улучшенная бейзлайн из библиотеки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
